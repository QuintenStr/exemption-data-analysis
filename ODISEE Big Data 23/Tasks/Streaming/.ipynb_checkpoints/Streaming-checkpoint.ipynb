{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "641bc648",
   "metadata": {},
   "source": [
    "# Streaming - Kafka\n",
    "\n",
    "**Deadline:10 mei om 23u59** \n",
    "\n",
    "In deze oefening wordt een streaming applicatie gemaakt door middel van het Kafka streaming platform.\n",
    "Deze opgave bestaat uit drie delen:\n",
    "* Ten eerste het installeren en opstarten van het Kafka platform\n",
    "* Ten tweede het schrijven van een applicatie dat data stuurt naar het Kafka platform\n",
    "* Ten derde een Spark applicatie dat de data verwerkt van het Kafka platform.\n",
    "\n",
    "Indien je data moet/wil bewaren op het hdfs, doe dit dan in de **Oefeningen/Streaming** folder en maak deze folder ook leeg voor je begint.\n",
    "\n",
    "Een kort overzicht van het werken met Kafka binnen python kan je [hier](https://towardsdatascience.com/kafka-python-explained-in-10-lines-of-code-800e3e07dad1) vinden.\n",
    "\n",
    "## Installatie en opstarten\n",
    "\n",
    "Ga eerst op zoek naar hoe je Kafka kan gebruiken in python. \n",
    "Zoek hierbij dus de nodige stappen op om Kafka te installeren.\n",
    "Beschrijf hieronder de te volgen stappen en extra applicaties die moeten geinstalleerd worden om kafka te kunnen installeren en starten.\n",
    "Ga ten slotte op zoek naar een python package om te werken met kafka.\n",
    "Zorg ervoor dat dit ook geinstalleerd is.\n",
    "**Let op:** Het is reeds geinstalleerd in de kafka folder in je home-directory dus moeten deze commando's niet uitgevoerd worden.\n",
    "\n",
    "Beschrijf hieronder de te volgen stappen:\n",
    "\n",
    "De te volgen stappen zijn:\n",
    "* **Stap 1:** Installeer benodigde Java Development Kit (JDK) (OpenJDK or Oracle JDK zijn twee voorbeelden)\n",
    "* **Stap 2:** Download Kafka en ZooKeeper van: \\\n",
    "Kafka: https://kafka.apache.org/downloads \\\n",
    "ZooKeeper: https://zookeeper.apache.org/releases.html\n",
    "* **Stap 3:** Uitpakken bestanden (vervang version door juiste versie die je gedownload hebt): \\\n",
    "Kafka: tar -xzf kafka_\\<version>\\.tgz \\\n",
    "ZooKeeper: tar -xzf zookeeper-\\<version>\\.tar.gz\n",
    "* **Stap 4:** Configuratie: \\\n",
    "Kafka: in config/server.propperties \\\n",
    "ZooKeeper: zoo_sample.cfg is een template van de config. Rename naar zoo.cfg en edit naar juiste gegevens.\n",
    "\n",
    "Indien er commando's moeten uitgevoerd worden in de terminal, plaats een kopie hiervan ook in de code cel hieronder en verwijs in commentaar naar de bijhorende stap.\n",
    "* Ja, alles van de installatie moet in terminal gebeuren behalve de pip install.\n",
    "\n",
    "* **Stap 5:** Installeer kafka-python package met pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779b572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installatie commando's\n",
    "pip install kafka-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac4a2f",
   "metadata": {},
   "source": [
    "Aangezien kafka reeds geinstalleerd is moeten bovenstaande commando's niet uitgevoerd worden.\n",
    "Echter moet kafka wel nog opgestart worden.\n",
    "Schrijf in de code-cell hieronder de nodige commandline commando's om kafka goed op te starten.\n",
    "**Voer deze commando's ook uit.**\n",
    "\n",
    "* **Stap 6:** Opstarten van ZooKeeper en Kafka server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b81aa3",
   "metadata": {},
   "source": [
    "Opstart commando's (in terminal)\n",
    "\n",
    "* **Stap 7:** Open terminal in de juiste folder: \\\n",
    "(base) bigdata@bigdata-VirtualBox:~/kafka/bin$\n",
    "\n",
    "* **Stap 8**: ZooKeeper server starten: \\\n",
    "./zookeeper-server-start.sh ../config/zookeeper.properties\n",
    "\n",
    "* **Stap 9**: Kafka Server startem: \\\n",
    "./kafka-server-start.sh ../config/server.properties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c77c91",
   "metadata": {},
   "source": [
    "Let zeker op voor de volgende zaken bij het gebruik van kafka:\n",
    "* Het delete.topic.enable attribuut staat op true in de file config/server.properties. Dit laat toe om test-topics te verwijderen.\n",
    "* Indien er bij het starten van de applicaties een foutmelding komt dat de poort reeds in gebruik is, kies dan een andere poort. Gebruik hiervoor de properties files in de config directory. Zorg er ook voor dat deze poort gekend is bij het starten van kafka door de configuratie aan te passen in de file **server.properties**. Deze poort heb je ook nodig bij het aanmaken van topics dus ga zeker op zoek naar welke ik geconfigureerd heb.\n",
    "\n",
    "**Indien je de poort hebt aangepast, welke poort kan gebruikt worden om met kafka te connecteren:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9177bfa",
   "metadata": {},
   "source": [
    "Indien je bij het opstarten van de applicaties foutmeldingen krijgt waar je niet direct het antwoord op weet, contacteer me en dan zoek ik samen met jou naar de oplossing.\n",
    "\n",
    "Als kafka correct opgestart is dan kunnen we de installatie testen om dit te verifieren.\n",
    "Voer daarvoor de volgende stappen uit:\n",
    "* Stap 1: Maak een nieuw topic \"test\" aan.\n",
    "* Stap 2: Print een lijst met alle beschikbare topics. (Plaats het nodige commando hieronder)\n",
    "* Stap 3: Print een beschrijving van het net aangemaakte topic\n",
    "\n",
    "Schrijf in de code-cellen hieronder de nodige commandline commando's om deze stappen uit te voeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cfa7c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaAdminClient, TopicPartition, KafkaConsumer\n",
    "from kafka.admin import NewTopic\n",
    "import json\n",
    "from IPython.display import JSON\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe60810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zookeeper server runnen\n",
    "command = \"cd ~/kafka/bin && ./zookeeper-server-start.sh ../config/zookeeper.properties\"\n",
    "os.system(\"gnome-terminal -- bash -c \\\"%s; exec bash\\\"\" % command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2d2109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# kafka server runnen\n",
    "command = \"cd ~/kafka/bin && ./kafka-server-start.sh ../config/server.properties\"\n",
    "os.system(\"gnome-terminal -- bash -c \\\"%s; exec bash\\\"\" % command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "686c0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bootstrap_servers = 'localhost:9092'\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02fd0304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No topics found in the Kafka cluster.\n"
     ]
    }
   ],
   "source": [
    "# alle topics verwijderen zodat er geen foutmelding komt van topic test already exists bij meerdere keren heel de\n",
    "# de notebook te runnen\n",
    "\n",
    "topic_names = admin_client.list_topics()\n",
    "if topic_names:\n",
    "    for topic_name in topic_names:\n",
    "        try:\n",
    "            admin_client.delete_topics(topics=[topic_name])\n",
    "            print(f'Topic \"{topic_name}\" has been marked for deletion.')\n",
    "        except TopicAlreadyMarkedForDeletionError:\n",
    "            print(f'Topic \"{topic_name}\" has already been marked for deletion.')\n",
    "else:\n",
    "    print('No topics found in the Kafka cluster.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "770c9850",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CreateTopicsResponse_v3(throttle_time_ms=0, topic_errors=[(topic='test', error_code=0, error_message=None)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# aanmaken topic test\n",
    "topic_name = 'test'\n",
    "\n",
    "new_topic = NewTopic(topic_name, num_partitions=1, replication_factor=1)\n",
    "admin_client.create_topics(new_topics=[new_topic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b76a2fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'test'}\n"
     ]
    }
   ],
   "source": [
    "# printen lijst met alle beschikbare topics\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers=bootstrap_servers)\n",
    "topics = consumer.topics()\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ed082cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"error_code\": 0,\n",
      "    \"topic\": \"test\",\n",
      "    \"is_internal\": false,\n",
      "    \"partitions\": [\n",
      "      {\n",
      "        \"error_code\": 0,\n",
      "        \"partition\": 0,\n",
      "        \"leader\": 0,\n",
      "        \"replicas\": [\n",
      "          0\n",
      "        ],\n",
      "        \"isr\": [\n",
      "          0\n",
      "        ],\n",
      "        \"offline_replicas\": []\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# printen informatie over test topic\n",
    "\n",
    "topic_metadata = admin_client.describe_topics()\n",
    "json_string = json.dumps(topic_metadata, indent=2)\n",
    "print(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ea449c",
   "metadata": {},
   "source": [
    "Voor er kan overgegaan worden naar het coderen van kafka applicaties, gaan we nog testen of er berichten kunnen verstuurd worden door de Kafka streaming server via de command terminal.\n",
    "Start hiervoor een console-producer applicatie en verstuur drie berichten. \n",
    "**Zorg ervoor dat er in minstens 1 van de berichten je naam staat.**\n",
    "\n",
    "Schrijf hieronder het gebruikte commando om de console producer op te starten."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc6224",
   "metadata": {},
   "source": [
    "Nadat de berichten verstuurd zijn, maak een consumer applicatie aan die alle berichten vanaf het begin van het topic uitleest. \n",
    "Maak een screenshot van de output met het commando erbij en sla dit op als **output_test.png**.\n",
    "\n",
    "Wanneer dit gebeurd is verwijder het test topic. Schrijf het commando hiervoor in de cell hieronder. Verifeer het commando voor het verwijderen van het topic door opnieuw een lijst af te printen met alle mogelijke beschikbare topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953470d",
   "metadata": {},
   "source": [
    "![Screenshot Output Test](output_test.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75208d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(topic_name, bootstrap_servers=bootstrap_servers, auto_offset_reset='earliest', group_id=None)\n",
    "\n",
    "for message in consumer:\n",
    "    value = message.value.decode('utf-8')\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "989cc0dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteTopicsResponse_v3(throttle_time_ms=0, topic_error_codes=[(topic='test', error_code=0)])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# delete topic commando\n",
    "\n",
    "admin_client.delete_topics(topics=[topic_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e75d3b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n"
     ]
    }
   ],
   "source": [
    "# controleer of het commando goed verwijderd is\n",
    "topics = consumer.topics()\n",
    "print(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b70bea",
   "metadata": {},
   "source": [
    "## Producer\n",
    "\n",
    "Download een zelfgekozen boek in txt formaat, bijvoorbeeld aan de hand van [deze link](https://www.gutenberg.org/).\n",
    "Schrijf nu een python producer applicatie in de code cell hieronder dat lijn per lijn dit boek wegschrijft als bytestring (door gebruik te maken van de .encode() functie) naar een nieuw topic met de naam \"BookStream\".\n",
    "Zorg ervoor dat er een seconde gewacht wordt tussen het versturen van opeenvolgende lijnen. \n",
    "(Bij het testen kan je de code vroegtuidig stoppen door de kernel te onderbreken).\n",
    "Door de file tag bovenaan wordt deze applicatie weggeschreven naar een python file.\n",
    "\n",
    "**Let op:** Je wilt niet dat bij het testen meerdere keren de start van het boek weggeschreven wordt naar het topic. Voorzie dus ook de nodige python code om het topic te verwijderen en opnieuw aan te maken bij het starten van de applicatie. \n",
    "Indien je bij het aanmaken en verwijderen van topics foutmeldingen krijgt kan het helpen om in de terminal te verifieren welke topics reeds bestaan en eventueel een delay toe te voegen voor/na de verschillende stappen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84df9d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kafka_producer.py\n"
     ]
    }
   ],
   "source": [
    "%%file kafka_producer.py\n",
    "# Kafka producer streaming book line by line\n",
    "\n",
    "import time\n",
    "from kafka import KafkaProducer, KafkaAdminClient\n",
    "from kafka.admin import NewTopic\n",
    "\n",
    "bookstream_topicname = 'BookStream'\n",
    "bootstrap_servers = 'localhost:9092'\n",
    "producer = KafkaProducer(bootstrap_servers=bootstrap_servers)\n",
    "admin_client = KafkaAdminClient(bootstrap_servers=bootstrap_servers)\n",
    "\n",
    "# probeer te deleten, als dat failed bestaat hij niet dus is ok, anders delete hij sws het topic\n",
    "topic_names = admin_client.list_topics()\n",
    "if topic_names:\n",
    "    for topic_name in topic_names:\n",
    "        if topic_name == bookstream_topicname:\n",
    "            print('Found an instance of BookStream topic. deleting...')\n",
    "            admin_client.delete_topics(topics=[bookstream_topicname])\n",
    "            \n",
    "# bugfix met een delay\n",
    "time.sleep(3)\n",
    "\n",
    "# maak daarna opnieuw een aan\n",
    "new_topic = NewTopic(bookstream_topicname, num_partitions=1, replication_factor=1)\n",
    "admin_client.create_topics(new_topics=[new_topic])\n",
    "print(f'BookStream topic made.')\n",
    "\n",
    "# bugfix met een delay\n",
    "time.sleep(3)\n",
    "\n",
    "print(f'Beginning streaming line per line.')\n",
    "\n",
    "# lijn per lijn met 1 seconde delay versturen\n",
    "file_path = './bookstream.txt'\n",
    "with open(file_path, \"r\") as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    if line:\n",
    "        print(bookstream_topicname, line.encode())\n",
    "        producer.send(bookstream_topicname, line.encode())\n",
    "        time.sleep(1)\n",
    "\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7eb2ee",
   "metadata": {},
   "source": [
    "Schrijf hieronder het nodige commando om de python applicatie te starten in de file kafka_producer.py\n",
    "Dit voer je echter best uit in een aparte terminal om deze file niet te blokkeren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdbe055b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = \"python kafka_producer.py\"\n",
    "os.system(\"gnome-terminal -- bash -c \\\"%s; exec bash\\\"\" % command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891df588",
   "metadata": {},
   "source": [
    "Voor we de weggeschreven data gaan gebruiken in een consumer kan je ook controleren of je producer het gewenste resultaat geeft door alle data in een bepaald topic uit te printen.\n",
    "Schrijf hieronder het terminal commando om dit uit te voeren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2292bd40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "﻿The Project Gutenberg eBook of The Benson Murder Case, by S. S. Van\n",
      "Dine\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook.\n",
      "Title: The Benson Murder Case\n",
      "Author: S. S. Van Dine\n",
      "Release Date: April 24, 2023 [eBook #70634]\n",
      "Language: English\n",
      "Produced by: Transcribed and produced by Brian Raiter\n",
      "*** START OF THE PROJECT GUTENBERG EBOOK THE BENSON MURDER CASE ***\n",
      "The Benson Murder Case\n",
      "by S. S. Van Dine\n",
      "[Frontispiece: Philo Vance, from a drawing by Herbert Stoops. A sketch\n",
      "of a seated gentleman in evening dress and wearing an monocle.]\n",
      "Contents\n",
      "I. Philo Vance at Home\n",
      "II. At the Scene of the Crime\n",
      "III. A Lady’s Hand-bag\n",
      "IV. The Housekeeper’s Story\n",
      "V. Gathering Information\n",
      "VI. Vance Offers an Opinion\n",
      "VII. Reports and an Interview\n",
      "VIII. Vance Accepts a Challenge\n",
      "IX. The Height of the Murderer\n",
      "X. Eliminating a Suspect\n",
      "XI. A Motive and a Threat\n",
      "XII. The Owner of a Colt-.45\n",
      "XIII. The Grey Cadillac\n",
      "XIV. Links in the Chain\n",
      "XV. “Pfyfe—Personal”\n",
      "XVI. Admissions and Suppressions\n",
      "XVII. The Forged Check\n",
      "XVIII. A Confession\n",
      "XIX. Vance Cross-examines\n",
      "XX. A Lady Explains\n",
      "XXI. Sartorial Revelations\n",
      "XXII. Vance Outlines a Theory\n",
      "XXIII. Checking an Alibi\n",
      "XXIV. The Arrest\n",
      "XXV. Vance Explains His Methods\n",
      "“Mr. Mason,” he said, “I wish to thank you for\n",
      "my life.”\n",
      "“Sir,” said Mason, “I had no interest in your\n",
      "life. The adjustment of your problem was the\n",
      "only thing of interest to me.”\n",
      "—Randolph Mason: Corrector of Destinies.\n",
      "Publisher’s Note\n",
      "It gives us considerable pleasure to be able to offer to the public\n",
      "the “inside” record of those of former District Attorney Markham’s\n",
      "criminal cases in which Mr. Philo Vance figured so effectively. The\n",
      "true inwardness of these famous cases has never before been revealed;\n",
      "for Mr. S. S. Van Dine, Mr. Vance’s lawyer and almost constant\n",
      "companion, being the only person who possessed a complete record of\n",
      "the facts, has only recently been permitted to make them public.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_7373/3206928050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconsumer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKafkaConsumer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'BookStream'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrap_servers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbootstrap_servers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_offset_reset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'earliest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36mnext_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_generator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m_message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_message_generator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consumer_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mrecord_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_offsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;31m# Generators are stateful, and it is possible that the tp / records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_records\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_offsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_offsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m_poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_to_next_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0;31m# after the long poll, we should check whether the group needs to rebalance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;31m# prior to returning data so that the group can stabilize faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    600\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# avoid negative timeouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;31m# called without the lock to avoid deadlock potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mstart_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0mend_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# controleer of de producer goed werkt\n",
    "consumer = KafkaConsumer('BookStream', bootstrap_servers=bootstrap_servers, auto_offset_reset='earliest', group_id=None)\n",
    "\n",
    "for message in consumer:\n",
    "    value = message.value.decode('utf-8')\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f69481",
   "metadata": {},
   "source": [
    "# Consumer\n",
    "\n",
    "Nadat je erin geslaagd bent om lijnen uit de boek naar een kafka-topic te schrijven, kan je verdergaan naar de volgende stap.\n",
    "Hierbij ga je een python-applicatie schrijven die data binnenkrijgt via een kafka stream en deze verwerkt door middel van spark structured streaming.\n",
    "Meer informatie over structured streaming kan je [hier](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html) vinden.\n",
    "Deze applicatie voert de volgende stapen uit:\n",
    "* Print het schema van de dataframes aangeleverd door Kafka\n",
    "* Zet de aangeleverde dataframes om naar een dataframe met de volgende kolommen:\n",
    " * Aantal karakters per lijn\n",
    " * Aantal keer dat elke letter voorkomt (enkel letters) en zorg ervoor dat je zowel hoofd- als kleine letters meetelt.\n",
    " * Aantal woorden in de lijn\n",
    "* Zorg ervoor dat dit dataframe na het verwerken van elke batch uitgeprint wordt in een console.\n",
    "* Zorg ervoor dat enkel nieuwe berichten in het topic behandeld worden.\n",
    "\n",
    "Opmerkingen:\n",
    "* Default wordt er steeds verder gelezen in het topic. Om elke keer dat je de consumer start, opnieuw te beginnen vanaf de start van het boek kan je de optie startingOffsets op earliest zetten.\n",
    "* Vergeet niet de vragen onder de code te beantwoorden!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0bfdced0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kafka_consumer.py\n"
     ]
    }
   ],
   "source": [
    "%%file kafka_consumer.py\n",
    "# Kafka consumer met pyspark om een charcount uit te voeren\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, size, split, trim, length, regexp_replace, expr\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamReader\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", 'localhost:9092') \\\n",
    "    .option(\"subscribe\", \"BookStream\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "print(\"Schema of incoming dataframe:\")\n",
    "kafka_df.printSchema()\n",
    "\n",
    "print(\"Contents of incoming dataframe:\")\n",
    "\n",
    "print(\"New dataframe with questions answered:\")\n",
    "new_df = kafka_df.select(\"value\")\n",
    "new_df = new_df.withColumn(\"value\", col(\"value\").cast(\"binary\").cast(\"string\"))\n",
    "new_df = new_df.withColumn(\"word_count\", size(split(trim(col(\"value\")), \" \")))\n",
    "new_df = new_df.withColumn(\"char_count\", length(col(\"value\")))\n",
    "\n",
    "for letter in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "    new_df = new_df.withColumn(f\"{letter.lower()}_lower_count\", length(trim(expr(f\"regexp_replace(value, '[^{letter.lower()}]', '')\"))))\n",
    "\n",
    "    new_df = new_df.withColumn(f\"{letter.upper()}_upper_count\", length(trim(expr(f\"regexp_replace(value, '[^{letter.upper()}]', '')\"))))\n",
    "\n",
    "query = new_df.writeStream \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"path\", \"~/bigdata/oefening-streaming-QuintenStr\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"1 second\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f6480a",
   "metadata": {},
   "source": [
    "De bovenstaande applicatie kan gestart worden door het volgende commando uit te voeren. Let op het --packages argument om kafka en pyspark te combineren. Let op dat dit een streaming applicatie is die actief blijft dus onderbreek de kernel om te hertesten of voer dit uit in een aparte terminal. \n",
    "Maak een screenshot van een deel van de output (na initialisatie) en sla het op onder de naam **Streaming_output_1.png**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bfb115b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# commando om consumer te starten\n",
    "command = \"python kafka_consumer.py\"\n",
    "os.system(\"gnome-terminal -- bash -c \\\"%s; exec bash\\\"\" % command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87516a",
   "metadata": {},
   "source": [
    "![Screenshot Output Stream 2](Streaming_output_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2224895",
   "metadata": {},
   "source": [
    "Screenshot is in kladblok op windows zo kan is het leesbaarder want in de terminal is het helemaal door elkaar geklutst :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590df940",
   "metadata": {},
   "source": [
    "**Vragen:**\n",
    "* Wat is het verschil tussen de append en complete methode om de data weg te schrijven? Toon hier een voorbeeld van op basis van de uitgewerkte oefening.\n",
    "* Stel dat we het totaal aantal keer dat elke letter voorkomt in de laatst geziene 20 seconden willen optellen en tonen. Wat moet er hiervoor aangepast worden in bovenstaande applicatie? Voer deze aanpassingen aan in de code cell hieronder.\n",
    "* We willen nu ook in de gegroepeerde data bijhouden hoeveel rijen er samengeteld werden. Voeg ook de aanpassing hiervoor toe in de code-cell hieronder. Wat moet er hiervoor aangepast worden in bovenstaande applicatie?\n",
    "\n",
    "Indien de aanpassingen niet lukken. Noteer hieronder zeker je ideeën om de aanpassingen door te voeren. \n",
    "Sla na de aanpassingen opnieuw een screenshot op van een deel van de output (na initialisatie) op onder de naam **Streaming_output_2.png**\n",
    "\n",
    "**Antwoorden:**\n",
    "* Vraag 1: Het verschil tussen de \"append\" en \"complete\" output modes in Spark Structured Streaming is dat \"append\" alleen nieuwe data naar de uitvoerbestemming schrijft, terwijl \"complete\" de volledige resultaatset, inclusief historische gegevens, naar de uitvoerbestemming schrijft. \"Append\" is handig als je alleen nieuwe data wilt opslaan, terwijl \"complete\" geschikt is als je een volledig historisch overzicht van alle gegevens wilt bijhouden. Ik heb geen aggreation in mijn code dus ik kan wel geen gebruik maken van complete.\n",
    "* Vraag 2:     .trigger(processingTime=\"20 seconds\")\n",
    "* Vraag 3: Omdat ik niet met gegroepeerde data werk kan ik simpelweg gewoon het aantal rijen in het dataframe uitprinten. Deze zijn soms meer of minder dan 20 door delays/cpu niet snel genoeg/eerste batch is alles omdat je met starting offset earliest zit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b374491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting kafka_consumer20sec.py\n"
     ]
    }
   ],
   "source": [
    "%%file kafka_consumer20sec.py\n",
    "# Kafka consumer met pyspark om een charcount uit te voeren\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, size, split, trim, length, regexp_replace, expr\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaStreamReader\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "kafka_df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", 'localhost:9092') \\\n",
    "    .option(\"subscribe\", \"BookStream\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    print(\"Schema of incoming dataframe:\")\n",
    "    df.printSchema()\n",
    "\n",
    "    print(\"Contents of incoming dataframe:\")\n",
    "    df.show(truncate=False)\n",
    "\n",
    "    print(\"New dataframe with questions answered:\")\n",
    "    new_df = df.select(\"value\")\n",
    "    new_df = new_df.withColumn(\"value\", col(\"value\").cast(\"binary\").cast(\"string\"))\n",
    "    new_df = new_df.withColumn(\"word_count\", size(split(trim(col(\"value\")), \" \")))\n",
    "    new_df = new_df.withColumn(\"char_count\", length(col(\"value\")))\n",
    "\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ':\n",
    "        new_df = new_df.withColumn(f\"{letter.lower()}_lower_count\", length(trim(expr(f\"regexp_replace(value, '[^{letter.lower()}]', '')\"))))\n",
    "\n",
    "        new_df = new_df.withColumn(f\"{letter.upper()}_upper_count\", length(trim(expr(f\"regexp_replace(value, '[^{letter.upper()}]', '')\"))))\n",
    "    \n",
    "    new_df.show(truncate=False)\n",
    "    \n",
    "    print(\"Aantal rijen opgeteld:\")\n",
    "    print(new_df.count())\n",
    "\n",
    "\n",
    "#new_df.show(truncate=False)\n",
    "\n",
    "query = kafka_df.writeStream \\\n",
    "    .trigger(processingTime=\"20 seconds\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f41a9ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# commando om consumer te starten\n",
    "command = \"python kafka_consumer20sec.py\"\n",
    "os.system(\"gnome-terminal -- bash -c \\\"%s; exec bash\\\"\" % command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2437f069",
   "metadata": {},
   "source": [
    "![Screenshot Output Stream 2](Streaming_output_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186a819",
   "metadata": {},
   "source": [
    "Screenshot is in kladblok op windows zo kan is het leesbaarder want in de terminal is het helemaal door elkaar geklutst :)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
